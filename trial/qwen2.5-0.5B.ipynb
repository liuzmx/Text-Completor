{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "def mask_random_phrase(text, mask_token=\"[MASK]\", max_mask_length=8):\n",
    "    words = text.split()\n",
    "    if len(words) == 0:\n",
    "        return text\n",
    "\n",
    "    start_index = random.randint(0, len(words) - 1)\n",
    "    end_index = min(start_index + random.randint(1, max_mask_length), len(words))\n",
    "\n",
    "    masked_words = words[:start_index] + [mask_token] + words[end_index:]\n",
    "    return \" \".join(masked_words)\n",
    "\n",
    "\n",
    "def process_content(content, num_masks=3):\n",
    "    sentences = sent_tokenize(content)\n",
    "    merged_sentences = []\n",
    "\n",
    "    for i in range(0, len(sentences), 2):\n",
    "        if i + 1 < len(sentences):\n",
    "            merged_text = sentences[i] + \" \" + sentences[i + 1]\n",
    "        else:\n",
    "            merged_text = sentences[i]\n",
    "\n",
    "        original_text = merged_text\n",
    "        for _ in range(num_masks):\n",
    "            masked_text = mask_random_phrase(original_text)\n",
    "            yield f\"{masked_text}\\t{original_text}\"\n",
    "\n",
    "\n",
    "def generate_dataset(input_file, output_file, num_masks=3):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as infile, open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        for line in infile:\n",
    "            data = json.loads(line)\n",
    "            content = data.get(\"title\", \"\") + \" \" + data.get(\"content\", \"\")\n",
    "            content = content.replace(\"\\n\", \" \")\n",
    "            for sample in process_content(content, num_masks=num_masks):\n",
    "                outfile.write(sample + \"\\n\")\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "generate_dataset(\"data/bbc_news/bbc_news_2024-11.jsonl\", \"train.txt\", num_masks=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/liuzhiming/.miniconda3/envs/text-completor/lib/python3.11/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ğŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1275' max='1275' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1275/1275 04:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.459400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.768100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./fine-tuned-qwen/tokenizer_config.json',\n",
       " './fine-tuned-qwen/special_tokens_map.json',\n",
       " './fine-tuned-qwen/vocab.json',\n",
       " './fine-tuned-qwen/merges.txt',\n",
       " './fine-tuned-qwen/added_tokens.json',\n",
       " './fine-tuned-qwen/tokenizer.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    TextDataset,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# Step 1: Load and preprocess the dataset\n",
    "def load_and_preprocess_data(file_path):\n",
    "    # Convert list of strings to a Hugging Face Dataset\n",
    "    dataset = TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=file_path,\n",
    "        block_size=128,\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Step 2: Set up the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "# Step 3: Initialize the model and tokenizer\n",
    "model_name =  \"/data/modelscope/Qwen2.5-0.5B\" # Replace with the correct model name if different\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Add special tokens if necessary\n",
    "special_tokens_dict = {\"additional_special_tokens\": [\"[MASK]\"]}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Step 4: Prepare the dataset\n",
    "file_path = \"train_1k.txt\"\n",
    "dataset = load_and_preprocess_data(file_path)\n",
    "\n",
    "# Step 5: Create data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "# Step 6: Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "# Step 7: Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Step 8: Save the model\n",
    "model.save_pretrained(\"./fine-tuned-qwen\")\n",
    "tokenizer.save_pretrained(\"./fine-tuned-qwen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Step 1: Load the tokenizer and model\n",
    "model_name_or_path = \"./fine-tuned-qwen\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define a function to generate text\n",
    "def generate_text(prompt, max_length=50):\n",
    "    # Encode the input prompt\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        prompt,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids']\n",
    "    attention_mask = encoding['attention_mask']\n",
    "    \n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "    \n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "prompt = \"This is a document. It contains some [MASK]\"\n",
    "generated_text = generate_text(prompt)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated Text: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œæˆ‘å’Œæœ‹å‹å»å…¬å›­æ•£æ­¥ï¼Œæˆ‘ä»¬çœ‹åˆ°äº† <MASK> å¾ˆå¤šäººåœ¨é‚£é‡Œç©è€ã€‚æœ‰çš„äººåœ¨æ ‘è«ä¸‹ä¹˜å‡‰ï¼Œæœ‰çš„äººåœ¨é‡åœ°é‡Œçƒ§çƒ¤ï¼Œè¿˜æœ‰çš„äººåœ¨æ‰è¿·è—ï¼Œçœ‹è°å…ˆçœ‹è§é‡‘é±¼ã€‚\tå¤ªé˜³å…‰è€€ç€å¤§åœ°ï¼Œä¸‡ç‰©ç”Ÿé•¿å¾—å¦‚æ­¤ä¹‹èŒ‚ç››ï¼Œæˆ‘å’Œæœ‹å‹å»å…¬å›­æ•£æ­¥ï¼Œ\n",
      "Predicted middle part: >\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# åŠ è½½é¢„è®­ç»ƒçš„tokenizerå’Œmodel\n",
    "# model_name = \"/data/modelscope/Qwen2.5-0.5B\"\n",
    "model_name = \"./fine-tuned-qwen\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# è¯·æ ¹æ®ä¸Šä¸‹æ–‡å¡«è¡¥ç¼ºå¤±çš„éƒ¨åˆ†ï¼š\n",
      "ä¸Šæ–‡: ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œæˆ‘å’Œæœ‹å‹å»å…¬å›­æ•£æ­¥ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†\n",
      "\n",
      "ä¸‹æ–‡: å¾ˆå¤šäººåœ¨é‚£é‡Œç©è€ã€‚æœ‰çš„äººåœ¨æ ‘è«ä¸‹ä¹˜å‡‰ï¼Œæœ‰çš„äººåœ¨é‡åœ°é‡Œçƒ§çƒ¤ï¼Œè¿˜æœ‰çš„äººåœ¨æ‰è¿·è—ï¼Œçœ‹è°å…ˆçœ‹è§é‡‘é±¼ã€‚å¤ªé˜³å…‰è€€ç€å¤§åœ°ï¼Œä¸‡ç‰©ç”Ÿé•¿å¾—å¦‚æ­¤ä¹‹èŒ‚ç››ï¼Œæˆ‘å’Œæœ‹å‹å»å…¬å›­æ•£æ­¥ï¼Œ\n",
      "ç»™å‡ºä¸Šä¸‹æ–‡é—´ç¼ºå¤±éƒ¨åˆ†ï¼šâ€œå¦‚æœå¤©æ°”ä¸ç»™åŠ›ï¼Œå°±æ²¡æœ‰è¿™åœºå®´å¸­äº†â€æ„æ€æ˜¯ï¼Œå¦‚æœè¿æ°”ä¸å¥½ï¼Œå°±æ²¡æœ‰è¿™åœº\n",
      "Predicted middle part: \n"
     ]
    }
   ],
   "source": [
    "def predict_missing_text(prefix, suffix):\n",
    "    # åˆ›å»ºä¸€ä¸ªç‰¹æ®Šçš„tokenè¡¨ç¤ºç¼ºå¤±çš„éƒ¨åˆ†\n",
    "    mask_token = \"[MASK]\"\n",
    "\n",
    "    # æ„å»ºè¾“å…¥åºåˆ—ï¼Œä½¿ç”¨æ¨¡æ¿å’Œæç¤º\n",
    "    input_sequence = (\n",
    "        f\"# è¯·æ ¹æ®ä¸Šä¸‹æ–‡å¡«è¡¥ç¼ºå¤±çš„éƒ¨åˆ†ï¼š\\nä¸Šæ–‡: {prefix}\\n\\nä¸‹æ–‡: {suffix}\\nç»™å‡ºä¸Šä¸‹æ–‡é—´ç¼ºå¤±éƒ¨åˆ†ï¼š\"\n",
    "    )\n",
    "\n",
    "    # ç¼–ç è¾“å…¥åºåˆ—\n",
    "    inputs = tokenizer(input_sequence, return_tensors=\"pt\")\n",
    "\n",
    "    # è·å–æ¨¡å‹çš„è¾“å‡º\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_new_tokens=20,  # è®¾ç½®ç”Ÿæˆçš„æ–°ä»¤ç‰Œæ•°é‡\n",
    "            num_return_sequences=1,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "        )\n",
    "\n",
    "    # è§£ç ç”Ÿæˆçš„æ–‡æœ¬\n",
    "    predicted_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(predicted_text)\n",
    "\n",
    "    # æå–é¢„æµ‹çš„ç¼ºå¤±éƒ¨åˆ†\n",
    "    start_index = predicted_text.find(suffix) + len(suffix)\n",
    "    end_index = predicted_text.rfind(\"\\n\", start_index)\n",
    "    if end_index == -1:\n",
    "        end_index = len(predicted_text)\n",
    "\n",
    "    predicted_middle_part = predicted_text[start_index:end_index].strip()\n",
    "\n",
    "    return predicted_middle_part\n",
    "\n",
    "\n",
    "# ç¤ºä¾‹ç”¨æ³•\n",
    "prefix = \"ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œæˆ‘å’Œæœ‹å‹å»å…¬å›­æ•£æ­¥ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†\"\n",
    "suffix = \"å¾ˆå¤šäººåœ¨é‚£é‡Œç©è€ã€‚æœ‰çš„äººåœ¨æ ‘è«ä¸‹ä¹˜å‡‰ï¼Œæœ‰çš„äººåœ¨é‡åœ°é‡Œçƒ§çƒ¤ï¼Œè¿˜æœ‰çš„äººåœ¨æ‰è¿·è—ï¼Œçœ‹è°å…ˆçœ‹è§é‡‘é±¼ã€‚å¤ªé˜³å…‰è€€ç€å¤§åœ°ï¼Œä¸‡ç‰©ç”Ÿé•¿å¾—å¦‚æ­¤ä¹‹èŒ‚ç››ï¼Œæˆ‘å’Œæœ‹å‹å»å…¬å›­æ•£æ­¥ï¼Œ\"\n",
    "\n",
    "predicted_middle_part = predict_missing_text(prefix, suffix)\n",
    "print(f\"Predicted middle part: {predicted_middle_part}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-completor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
