{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 1\n",
      "Device 0: NVIDIA GeForce RTX 4090\n",
      "trainable params: 1,081,344 || all params: 494,872,192 || trainable%: 0.2185\n",
      "None\n",
      "Skipping malformed line: [MASK] : \"Forbidden\" song haunts South Korea's Suneung students - BBC News.Suneung, an eight-hour university placement exam billed as one of the toughest in the world, kicked off on Thursday  A brief yearly silence has once again enveloped South Korea, as half a million students across the country sit for the most important test of their lives.\tAPT.\n",
      "Skipping malformed line: Inside Murder Trial - A Deadly Affair - 1.Well Ablaze - BBC Sounds. [MASK]\t5.\n",
      "Epoch 1/3\n",
      "Step 0, Loss: 21.33559226989746\n",
      "Step 10, Loss: 20.634693145751953\n",
      "Step 20, Loss: 19.510395050048828\n",
      "Step 30, Loss: 18.83188247680664\n",
      "Step 40, Loss: 17.6241455078125\n",
      "Step 50, Loss: 15.809399604797363\n",
      "Step 60, Loss: 13.600440979003906\n",
      "Step 70, Loss: 12.043617248535156\n",
      "Step 80, Loss: 11.139253616333008\n",
      "Step 90, Loss: 10.368639945983887\n",
      "Step 100, Loss: 10.17837905883789\n",
      "Step 110, Loss: 10.025651931762695\n",
      "Step 120, Loss: 9.362980842590332\n",
      "Step 130, Loss: 9.13856315612793\n",
      "Step 140, Loss: 8.948484420776367\n",
      "Step 150, Loss: 9.421664237976074\n",
      "Step 160, Loss: 8.738255500793457\n",
      "Step 170, Loss: 8.667190551757812\n",
      "Step 180, Loss: 8.600587844848633\n",
      "Step 190, Loss: 8.465675354003906\n",
      "Step 200, Loss: 8.776036262512207\n",
      "Step 210, Loss: 8.414793968200684\n",
      "Step 220, Loss: 8.425274848937988\n",
      "Step 230, Loss: 8.478426933288574\n",
      "Step 240, Loss: 8.459212303161621\n",
      "Step 250, Loss: 8.311088562011719\n",
      "Step 260, Loss: 8.410654067993164\n",
      "Step 270, Loss: 8.371072769165039\n",
      "Step 280, Loss: 8.279635429382324\n",
      "Step 290, Loss: 8.274041175842285\n",
      "Step 300, Loss: 8.346528053283691\n",
      "Step 310, Loss: 8.275461196899414\n",
      "Step 320, Loss: 8.239350318908691\n",
      "Step 330, Loss: 8.238348960876465\n",
      "Step 340, Loss: 8.22517204284668\n",
      "Step 350, Loss: 8.348977088928223\n",
      "Step 360, Loss: 8.210714340209961\n",
      "Step 370, Loss: 8.214495658874512\n",
      "Step 380, Loss: 8.199930191040039\n",
      "Step 390, Loss: 8.241579055786133\n",
      "Step 400, Loss: 8.302019119262695\n",
      "Step 410, Loss: 8.200759887695312\n",
      "Step 420, Loss: 8.193602561950684\n",
      "Step 430, Loss: 8.222283363342285\n",
      "Step 440, Loss: 8.218033790588379\n",
      "Step 450, Loss: 8.210132598876953\n",
      "Step 460, Loss: 8.16148567199707\n",
      "Step 470, Loss: 8.206552505493164\n",
      "Step 480, Loss: 8.199217796325684\n",
      "Step 490, Loss: 8.151496887207031\n",
      "Step 500, Loss: 8.149133682250977\n",
      "Step 510, Loss: 8.135709762573242\n",
      "Step 520, Loss: 8.141707420349121\n",
      "Step 530, Loss: 8.170309066772461\n",
      "Step 540, Loss: 8.142547607421875\n",
      "Step 550, Loss: 8.172430038452148\n",
      "Step 560, Loss: 8.12399673461914\n",
      "Step 570, Loss: 8.125432014465332\n",
      "Step 580, Loss: 8.221761703491211\n",
      "Step 590, Loss: 8.119351387023926\n",
      "Step 600, Loss: 8.1866455078125\n",
      "Step 610, Loss: 8.112325668334961\n",
      "Step 620, Loss: 8.145721435546875\n",
      "Step 630, Loss: 8.183305740356445\n",
      "Step 640, Loss: 8.104316711425781\n",
      "Step 650, Loss: 8.109334945678711\n",
      "Step 660, Loss: 8.163558959960938\n",
      "Step 670, Loss: 8.095609664916992\n",
      "Step 680, Loss: 8.100340843200684\n",
      "Step 690, Loss: 8.096118927001953\n",
      "Step 700, Loss: 8.092582702636719\n",
      "Step 710, Loss: 8.147850036621094\n",
      "Step 720, Loss: 8.086318969726562\n",
      "Step 730, Loss: 8.08413028717041\n",
      "Step 740, Loss: 8.078458786010742\n",
      "Step 750, Loss: 8.113627433776855\n",
      "Step 760, Loss: 8.104485511779785\n",
      "Step 770, Loss: 8.14449405670166\n",
      "Step 780, Loss: 8.116247177124023\n",
      "Step 790, Loss: 8.088790893554688\n",
      "Step 800, Loss: 8.070993423461914\n",
      "Step 810, Loss: 8.074131965637207\n",
      "Step 820, Loss: 8.138898849487305\n",
      "Step 830, Loss: 8.062844276428223\n",
      "Epoch 2/3\n",
      "Step 0, Loss: 8.079898834228516\n",
      "Step 10, Loss: 8.099286079406738\n",
      "Step 20, Loss: 8.09216022491455\n",
      "Step 30, Loss: 8.065040588378906\n",
      "Step 40, Loss: 8.065753936767578\n",
      "Step 50, Loss: 8.088560104370117\n",
      "Step 60, Loss: 8.060434341430664\n",
      "Step 70, Loss: 8.092429161071777\n",
      "Step 80, Loss: 8.082656860351562\n",
      "Step 90, Loss: 8.140241622924805\n",
      "Step 100, Loss: 8.10467529296875\n",
      "Step 110, Loss: 8.0894775390625\n",
      "Step 120, Loss: 8.10420036315918\n",
      "Step 130, Loss: 8.057967185974121\n",
      "Step 140, Loss: 8.052946090698242\n",
      "Step 150, Loss: 8.061773300170898\n",
      "Step 160, Loss: 8.054211616516113\n",
      "Step 170, Loss: 8.059616088867188\n",
      "Step 180, Loss: 8.100172996520996\n",
      "Step 190, Loss: 8.10650634765625\n",
      "Step 200, Loss: 8.042613983154297\n",
      "Step 210, Loss: 8.049768447875977\n",
      "Step 220, Loss: 8.100272178649902\n",
      "Step 230, Loss: 8.076147079467773\n",
      "Step 240, Loss: 8.072321891784668\n",
      "Step 250, Loss: 8.053518295288086\n",
      "Step 260, Loss: 8.04907512664795\n",
      "Step 270, Loss: 8.042972564697266\n",
      "Step 280, Loss: 8.06894302368164\n",
      "Step 290, Loss: 8.068473815917969\n",
      "Step 300, Loss: 8.043118476867676\n",
      "Step 310, Loss: 8.070327758789062\n",
      "Step 320, Loss: 8.042305946350098\n",
      "Step 330, Loss: 8.066368103027344\n",
      "Step 340, Loss: 8.058038711547852\n",
      "Step 350, Loss: 8.06489372253418\n",
      "Step 360, Loss: 8.041496276855469\n",
      "Step 370, Loss: 8.06630802154541\n",
      "Step 380, Loss: 8.064692497253418\n",
      "Step 390, Loss: 8.064691543579102\n",
      "Step 400, Loss: 8.081625938415527\n",
      "Step 410, Loss: 8.057876586914062\n",
      "Step 420, Loss: 8.033247947692871\n",
      "Step 430, Loss: 8.08525276184082\n",
      "Step 440, Loss: 8.055516242980957\n",
      "Step 450, Loss: 8.035999298095703\n",
      "Step 460, Loss: 8.037153244018555\n",
      "Step 470, Loss: 8.030290603637695\n",
      "Step 480, Loss: 8.071662902832031\n",
      "Step 490, Loss: 8.034250259399414\n",
      "Step 500, Loss: 8.033097267150879\n",
      "Step 510, Loss: 8.07303237915039\n",
      "Step 520, Loss: 8.05089282989502\n",
      "Step 530, Loss: 8.048001289367676\n",
      "Step 540, Loss: 8.032630920410156\n",
      "Step 550, Loss: 8.029630661010742\n",
      "Step 560, Loss: 8.032350540161133\n",
      "Step 570, Loss: 8.022583961486816\n",
      "Step 580, Loss: 8.047962188720703\n",
      "Step 590, Loss: 8.066659927368164\n",
      "Step 600, Loss: 8.029593467712402\n",
      "Step 610, Loss: 8.046568870544434\n",
      "Step 620, Loss: 8.047371864318848\n",
      "Step 630, Loss: 8.0269775390625\n",
      "Step 640, Loss: 8.046582221984863\n",
      "Step 650, Loss: 8.021858215332031\n",
      "Step 660, Loss: 8.04719066619873\n",
      "Step 670, Loss: 8.042313575744629\n",
      "Step 680, Loss: 8.0234375\n",
      "Step 690, Loss: 8.086313247680664\n",
      "Step 700, Loss: 8.067010879516602\n",
      "Step 710, Loss: 8.023236274719238\n",
      "Step 720, Loss: 8.042917251586914\n",
      "Step 730, Loss: 8.065532684326172\n",
      "Step 740, Loss: 8.018777847290039\n",
      "Step 750, Loss: 8.048724174499512\n",
      "Step 760, Loss: 8.041488647460938\n",
      "Step 770, Loss: 8.039165496826172\n",
      "Step 780, Loss: 8.018095016479492\n",
      "Step 790, Loss: 8.041136741638184\n",
      "Step 800, Loss: 8.037901878356934\n",
      "Step 810, Loss: 8.021003723144531\n",
      "Step 820, Loss: 8.020722389221191\n",
      "Step 830, Loss: 8.02283763885498\n",
      "Epoch 3/3\n",
      "Step 0, Loss: 8.060404777526855\n",
      "Step 10, Loss: 8.023433685302734\n",
      "Step 20, Loss: 8.037017822265625\n",
      "Step 30, Loss: 8.059666633605957\n",
      "Step 40, Loss: 8.040748596191406\n",
      "Step 50, Loss: 8.040943145751953\n",
      "Step 60, Loss: 8.033753395080566\n",
      "Step 70, Loss: 8.040343284606934\n",
      "Step 80, Loss: 8.051119804382324\n",
      "Step 90, Loss: 8.016630172729492\n",
      "Step 100, Loss: 8.012319564819336\n",
      "Step 110, Loss: 8.041102409362793\n",
      "Step 120, Loss: 8.03434944152832\n",
      "Step 130, Loss: 8.017217636108398\n",
      "Step 140, Loss: 8.033488273620605\n",
      "Step 150, Loss: 8.007657051086426\n",
      "Step 160, Loss: 8.034483909606934\n",
      "Step 170, Loss: 8.013827323913574\n",
      "Step 180, Loss: 8.041789054870605\n",
      "Step 190, Loss: 8.032527923583984\n",
      "Step 200, Loss: 8.013891220092773\n",
      "Step 210, Loss: 8.01253890991211\n",
      "Step 220, Loss: 8.032968521118164\n",
      "Step 230, Loss: 8.076377868652344\n",
      "Step 240, Loss: 8.014469146728516\n",
      "Step 250, Loss: 8.014760971069336\n",
      "Step 260, Loss: 8.016435623168945\n",
      "Step 270, Loss: 8.075467109680176\n",
      "Step 280, Loss: 8.018035888671875\n",
      "Step 290, Loss: 8.050726890563965\n",
      "Step 300, Loss: 8.04085922241211\n",
      "Step 310, Loss: 8.035115242004395\n",
      "Step 320, Loss: 8.010722160339355\n",
      "Step 330, Loss: 8.017316818237305\n",
      "Step 340, Loss: 8.030278205871582\n",
      "Step 350, Loss: 8.013436317443848\n",
      "Step 360, Loss: 8.034162521362305\n",
      "Step 370, Loss: 8.03482437133789\n",
      "Step 380, Loss: 8.009124755859375\n",
      "Step 390, Loss: 8.011754989624023\n",
      "Step 400, Loss: 8.009647369384766\n",
      "Step 410, Loss: 8.014183044433594\n",
      "Step 420, Loss: 8.031434059143066\n",
      "Step 430, Loss: 8.007122039794922\n",
      "Step 440, Loss: 8.029552459716797\n",
      "Step 450, Loss: 8.009621620178223\n",
      "Step 460, Loss: 8.02943229675293\n",
      "Step 470, Loss: 8.009464263916016\n",
      "Step 480, Loss: 8.011381149291992\n",
      "Step 490, Loss: 8.01058578491211\n",
      "Step 500, Loss: 8.049259185791016\n",
      "Step 510, Loss: 8.007333755493164\n",
      "Step 520, Loss: 8.008382797241211\n",
      "Step 530, Loss: 8.04718017578125\n",
      "Step 540, Loss: 8.030770301818848\n",
      "Step 550, Loss: 8.012514114379883\n",
      "Step 560, Loss: 8.010187149047852\n",
      "Step 570, Loss: 8.008153915405273\n",
      "Step 580, Loss: 8.011865615844727\n",
      "Step 590, Loss: 8.026819229125977\n",
      "Step 600, Loss: 8.069765090942383\n",
      "Step 610, Loss: 8.027481079101562\n",
      "Step 620, Loss: 8.029224395751953\n",
      "Step 630, Loss: 8.030261993408203\n",
      "Step 640, Loss: 8.046868324279785\n",
      "Step 650, Loss: 8.009689331054688\n",
      "Step 660, Loss: 8.010417938232422\n",
      "Step 670, Loss: 8.006200790405273\n",
      "Step 680, Loss: 8.029001235961914\n",
      "Step 690, Loss: 8.027971267700195\n",
      "Step 700, Loss: 8.009757995605469\n",
      "Step 710, Loss: 8.005367279052734\n",
      "Step 720, Loss: 8.006568908691406\n",
      "Step 730, Loss: 8.090991973876953\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 123\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m    122\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m--> 123\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m    125\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/data/liuzhiming/.miniconda3/envs/text-completor/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/liuzhiming/.miniconda3/envs/text-completor/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/data/liuzhiming/.miniconda3/envs/text-completor/lib/python3.11/site-packages/peft/peft_model.py:1719\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1718\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1719\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1720\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1725\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1726\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1728\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1730\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1732\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m/data/liuzhiming/.miniconda3/envs/text-completor/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/liuzhiming/.miniconda3/envs/text-completor/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/data/liuzhiming/.miniconda3/envs/text-completor/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:197\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/liuzhiming/.miniconda3/envs/text-completor/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:1165\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1162\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1165\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1178\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1179\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m/data/liuzhiming/.miniconda3/envs/text-completor/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/liuzhiming/.miniconda3/envs/text-completor/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/data/liuzhiming/.miniconda3/envs/text-completor/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:895\u001b[0m, in \u001b[0;36mQwen2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    883\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    884\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    885\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    892\u001b[0m         position_embeddings,\n\u001b[1;32m    893\u001b[0m     )\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 895\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    906\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/data/liuzhiming/.miniconda3/envs/text-completor/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/liuzhiming/.miniconda3/envs/text-completor/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/data/liuzhiming/.miniconda3/envs/text-completor/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:620\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;124;03m    hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;124;03m        into the model\u001b[39;00m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    618\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 620\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m    623\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    624\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    625\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    631\u001b[0m     position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[1;32m    632\u001b[0m )\n",
      "File \u001b[0;32m/data/liuzhiming/.miniconda3/envs/text-completor/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/liuzhiming/.miniconda3/envs/text-completor/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/data/liuzhiming/.miniconda3/envs/text-completor/lib/python3.11/site-packages/transformers/models/qwen2/modeling_qwen2.py:81\u001b[0m, in \u001b[0;36mQwen2RMSNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m     79\u001b[0m variance \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     80\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrsqrt(variance \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariance_epsilon)\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m \u001b[38;5;241m*\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(input_dtype)\n",
      "File \u001b[0;32m/data/liuzhiming/.miniconda3/envs/text-completor/lib/python3.11/site-packages/torch/nn/modules/module.py:1918\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1909\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1911\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[1;32m   1912\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[1;32m   1913\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[0;32m-> 1918\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1919\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1920\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import pandas as pd\n",
    "\n",
    "# 检查可用的GPU设备\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {num_gpus}\")\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    device = torch.device(\"cuda:0\")  # 使用第一个可用的GPU设备\n",
    "else:\n",
    "    print(\"No GPU devices available. Using CPU.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# 加载预训练模型和分词器\n",
    "model_name = \"/data/modelscope/Qwen2.5-0.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 确保分词器支持 [MASK] 令牌\n",
    "if tokenizer.mask_token is None:\n",
    "    tokenizer.add_special_tokens({\"mask_token\": \"[MASK]\"})\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# 调整模型以适应新的特殊令牌\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# 设置设备\n",
    "model.to(device)\n",
    "\n",
    "# 定义LORA配置\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "# 将LORA应用到模型上\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(model.print_trainable_parameters())\n",
    "\n",
    "\n",
    "# 自定义数据集类\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, examples, tokenizer, max_length=512):\n",
    "        self.examples = []\n",
    "        for example in examples:\n",
    "            parts = example.strip().split(\"\\t\")\n",
    "            if len(parts) == 2:\n",
    "                context_before_mask, missing_text_and_context_after = parts\n",
    "                context_before, _, context_after = context_before_mask.partition(\"[MASK]\")\n",
    "                if \" \" in missing_text_and_context_after:\n",
    "                    missing_text, context_after = missing_text_and_context_after.split(\" \", 1)\n",
    "                    self.examples.append((context_before, missing_text, context_after))\n",
    "                else:\n",
    "                    print(f\"Skipping malformed line: {example.strip()}\")\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context_before, missing_text, context_after = self.examples[idx]\n",
    "\n",
    "        # 创建完整的上下文\n",
    "        full_context = f\"{context_before} [MASK] {context_after}\"\n",
    "        input_ids = self.tokenizer.encode(\n",
    "            full_context, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\"\n",
    "        ).squeeze()\n",
    "\n",
    "        # 创建标签（即中间缺失的文本）\n",
    "        labels = [-100] * len(input_ids)\n",
    "        try:\n",
    "            mask_start = input_ids.tolist().index(self.tokenizer.mask_token_id)\n",
    "            mask_end = mask_start + len(missing_text.split())\n",
    "            for i in range(mask_start, mask_end):\n",
    "                labels[i] = input_ids[i]\n",
    "        except ValueError:\n",
    "            print(f\"Mask token not found in example: {full_context}\")\n",
    "            raise\n",
    "\n",
    "        attention_mask = (input_ids != self.tokenizer.pad_token_id).long()\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": torch.tensor(labels)}\n",
    "\n",
    "\n",
    "# 加载自定义数据集\n",
    "dataset_path = \"train2_10k.txt\"\n",
    "\n",
    "# 分割数据集为训练集和验证集\n",
    "with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "train_lines = lines[: int(0.8 * len(lines))]\n",
    "eval_lines = lines[int(0.8 * len(lines)) :]\n",
    "\n",
    "train_dataset = CustomDataset(train_lines, tokenizer)\n",
    "eval_dataset = CustomDataset(eval_lines, tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=8)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=8)\n",
    "\n",
    "# 设置优化器和学习率调度器\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=5e-5)\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# 训练循环\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print(f\"Step {step}, Loss: {loss.item()}\")\n",
    "\n",
    "# 评估模型\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "with torch.no_grad():\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        total_loss += outputs.loss.item()\n",
    "\n",
    "avg_eval_loss = total_loss / len(eval_dataloader)\n",
    "print(f\"Evaluation Loss: {avg_eval_loss}\")\n",
    "\n",
    "# 保存模型\n",
    "model.save_pretrained(\"./lora_qwen_2.5-0.5B_custom_fill_gpu0\")\n",
    "tokenizer.save_pretrained(\"./lora_qwen_2.5-0.5B_custom_fill_gpu0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 2\n",
      "Device 0: NVIDIA GeForce RTX 4090\n",
      "Device 1: NVIDIA GeForce RTX 4090\n",
      "trainable params: 540,672 || all params: 494,331,520 || trainable%: 0.1094\n",
      "None\n",
      "Skipping malformed line: [MASK] : \"Forbidden\" song haunts South Korea's Suneung students - BBC News.Suneung, an eight-hour university placement exam billed as one of the toughest in the world, kicked off on Thursday  A brief yearly silence has once again enveloped South Korea, as half a million students across the country sit for the most important test of their lives.\tAPT.\n",
      "Skipping malformed line: Inside Murder Trial - A Deadly Affair - 1.Well Ablaze - BBC Sounds. [MASK]\t5.\n",
      "Epoch 1/5\n",
      "Step 0, Loss: 20.749778747558594\n",
      "Step 10, Loss: 21.54783821105957\n",
      "Step 20, Loss: 21.47522735595703\n",
      "Step 30, Loss: 20.945316314697266\n",
      "Step 40, Loss: 21.52081871032715\n",
      "Step 50, Loss: 21.945276260375977\n",
      "Step 60, Loss: 20.522262573242188\n",
      "Step 70, Loss: 22.00442886352539\n",
      "Step 80, Loss: 21.065034866333008\n",
      "Step 90, Loss: 20.418201446533203\n",
      "Step 100, Loss: 20.578964233398438\n",
      "Step 110, Loss: 19.66363525390625\n",
      "Step 120, Loss: 20.500825881958008\n",
      "Step 130, Loss: 20.050809860229492\n",
      "Step 140, Loss: 19.736751556396484\n",
      "Step 150, Loss: 19.078510284423828\n",
      "Step 160, Loss: 19.598064422607422\n",
      "Step 170, Loss: 19.253543853759766\n",
      "Step 180, Loss: 19.061540603637695\n",
      "Step 190, Loss: 18.94033432006836\n",
      "Step 200, Loss: 18.827686309814453\n",
      "Step 210, Loss: 18.673236846923828\n",
      "Step 220, Loss: 18.19403839111328\n",
      "Step 230, Loss: 18.295082092285156\n",
      "Step 240, Loss: 17.533199310302734\n",
      "Step 250, Loss: 17.81730079650879\n",
      "Step 260, Loss: 17.453725814819336\n",
      "Step 270, Loss: 17.051551818847656\n",
      "Step 280, Loss: 16.084789276123047\n",
      "Step 290, Loss: 16.590587615966797\n",
      "Step 300, Loss: 15.991303443908691\n",
      "Step 310, Loss: 16.036888122558594\n",
      "Step 320, Loss: 14.797626495361328\n",
      "Step 330, Loss: 14.476726531982422\n",
      "Step 340, Loss: 14.178618431091309\n",
      "Step 350, Loss: 14.748414039611816\n",
      "Step 360, Loss: 13.451004028320312\n",
      "Step 370, Loss: 13.981483459472656\n",
      "Step 380, Loss: 12.823217391967773\n",
      "Step 390, Loss: 13.557622909545898\n",
      "Step 400, Loss: 12.377304077148438\n",
      "Step 410, Loss: 12.220674514770508\n",
      "Step 420, Loss: 12.831934928894043\n",
      "Step 430, Loss: 13.13919734954834\n",
      "Step 440, Loss: 12.480249404907227\n",
      "Step 450, Loss: 12.600788116455078\n",
      "Step 460, Loss: 11.950432777404785\n",
      "Step 470, Loss: 12.044926643371582\n",
      "Step 480, Loss: 11.467921257019043\n",
      "Step 490, Loss: 11.238804817199707\n",
      "Step 500, Loss: 11.302153587341309\n",
      "Step 510, Loss: 11.10195255279541\n",
      "Step 520, Loss: 10.94927978515625\n",
      "Step 530, Loss: 11.798555374145508\n",
      "Step 540, Loss: 11.420985221862793\n",
      "Step 550, Loss: 10.801977157592773\n",
      "Step 560, Loss: 11.641475677490234\n",
      "Step 570, Loss: 10.76031494140625\n",
      "Step 580, Loss: 10.19801139831543\n",
      "Step 590, Loss: 10.549023628234863\n",
      "Step 600, Loss: 11.06707763671875\n",
      "Step 610, Loss: 10.337328910827637\n",
      "Step 620, Loss: 10.510099411010742\n",
      "Step 630, Loss: 10.207249641418457\n",
      "Step 640, Loss: 10.233860969543457\n",
      "Step 650, Loss: 10.573539733886719\n",
      "Step 660, Loss: 10.129268646240234\n",
      "Step 670, Loss: 10.28038215637207\n",
      "Step 680, Loss: 10.069236755371094\n",
      "Step 690, Loss: 9.693562507629395\n",
      "Step 700, Loss: 10.1355562210083\n",
      "Step 710, Loss: 9.992817878723145\n",
      "Step 720, Loss: 9.877811431884766\n",
      "Step 730, Loss: 9.467466354370117\n",
      "Step 740, Loss: 9.862194061279297\n",
      "Step 750, Loss: 9.373186111450195\n",
      "Step 760, Loss: 9.450242042541504\n",
      "Step 770, Loss: 9.64815616607666\n",
      "Step 780, Loss: 9.77328109741211\n",
      "Step 790, Loss: 9.242522239685059\n",
      "Step 800, Loss: 9.596311569213867\n",
      "Step 810, Loss: 9.252374649047852\n",
      "Step 820, Loss: 9.229641914367676\n",
      "Step 830, Loss: 9.432491302490234\n",
      "Epoch 2/5\n",
      "Step 0, Loss: 9.131593704223633\n",
      "Step 10, Loss: 9.200517654418945\n",
      "Step 20, Loss: 9.091118812561035\n",
      "Step 30, Loss: 9.347661972045898\n",
      "Step 40, Loss: 9.643082618713379\n",
      "Step 50, Loss: 9.05221939086914\n",
      "Step 60, Loss: 9.018783569335938\n",
      "Step 70, Loss: 9.398404121398926\n",
      "Step 80, Loss: 9.25268840789795\n",
      "Step 90, Loss: 9.257516860961914\n",
      "Step 100, Loss: 9.002094268798828\n",
      "Step 110, Loss: 9.197158813476562\n",
      "Step 120, Loss: 8.8675537109375\n",
      "Step 130, Loss: 8.92941951751709\n",
      "Step 140, Loss: 8.834179878234863\n",
      "Step 150, Loss: 9.163971900939941\n",
      "Step 160, Loss: 8.861658096313477\n",
      "Step 170, Loss: 8.784674644470215\n",
      "Step 180, Loss: 9.01719856262207\n",
      "Step 190, Loss: 9.338894844055176\n",
      "Step 200, Loss: 9.03219985961914\n",
      "Step 210, Loss: 9.053739547729492\n",
      "Step 220, Loss: 8.73636531829834\n",
      "Step 230, Loss: 8.960192680358887\n",
      "Step 240, Loss: 8.74914264678955\n",
      "Step 250, Loss: 9.220580101013184\n",
      "Step 260, Loss: 9.05393123626709\n",
      "Step 270, Loss: 8.647327423095703\n",
      "Step 280, Loss: 8.710535049438477\n",
      "Step 290, Loss: 9.168048858642578\n",
      "Step 300, Loss: 8.884904861450195\n",
      "Step 310, Loss: 8.911240577697754\n",
      "Step 320, Loss: 9.44497299194336\n",
      "Step 330, Loss: 8.649450302124023\n",
      "Step 340, Loss: 8.995159149169922\n",
      "Step 350, Loss: 8.80488109588623\n",
      "Step 360, Loss: 8.860464096069336\n",
      "Step 370, Loss: 8.598968505859375\n",
      "Step 380, Loss: 8.795589447021484\n",
      "Step 390, Loss: 8.59326171875\n",
      "Step 400, Loss: 8.592168807983398\n",
      "Step 410, Loss: 9.248815536499023\n",
      "Step 420, Loss: 8.749536514282227\n",
      "Step 430, Loss: 8.549631118774414\n",
      "Step 440, Loss: 8.613327026367188\n",
      "Step 450, Loss: 8.562504768371582\n",
      "Step 460, Loss: 9.010184288024902\n",
      "Step 470, Loss: 8.71133804321289\n",
      "Step 480, Loss: 8.74416732788086\n",
      "Step 490, Loss: 8.529369354248047\n",
      "Step 500, Loss: 8.91629695892334\n",
      "Step 510, Loss: 8.895200729370117\n",
      "Step 520, Loss: 8.469371795654297\n",
      "Step 530, Loss: 8.705521583557129\n",
      "Step 540, Loss: 8.501106262207031\n",
      "Step 550, Loss: 8.468661308288574\n",
      "Step 560, Loss: 8.461099624633789\n",
      "Step 570, Loss: 8.486547470092773\n",
      "Step 580, Loss: 8.451361656188965\n",
      "Step 590, Loss: 8.433746337890625\n",
      "Step 600, Loss: 8.842658042907715\n",
      "Step 610, Loss: 8.797324180603027\n",
      "Step 620, Loss: 8.626628875732422\n",
      "Step 630, Loss: 8.832603454589844\n",
      "Step 640, Loss: 8.443832397460938\n",
      "Step 650, Loss: 8.407585144042969\n",
      "Step 660, Loss: 8.424676895141602\n",
      "Step 670, Loss: 8.439109802246094\n",
      "Step 680, Loss: 8.44067096710205\n",
      "Step 690, Loss: 8.429051399230957\n",
      "Step 700, Loss: 8.591140747070312\n",
      "Step 710, Loss: 8.424616813659668\n",
      "Step 720, Loss: 8.541092872619629\n",
      "Step 730, Loss: 8.383102416992188\n",
      "Step 740, Loss: 8.389876365661621\n",
      "Step 750, Loss: 8.414127349853516\n",
      "Step 760, Loss: 8.41473388671875\n",
      "Step 770, Loss: 8.576459884643555\n",
      "Step 780, Loss: 8.391552925109863\n",
      "Step 790, Loss: 8.377791404724121\n",
      "Step 800, Loss: 8.397019386291504\n",
      "Step 810, Loss: 8.369988441467285\n",
      "Step 820, Loss: 8.364599227905273\n",
      "Step 830, Loss: 8.682869911193848\n",
      "Epoch 3/5\n",
      "Step 0, Loss: 8.515851974487305\n",
      "Step 10, Loss: 8.50395679473877\n",
      "Step 20, Loss: 8.359169006347656\n",
      "Step 30, Loss: 8.357258796691895\n",
      "Step 40, Loss: 8.351460456848145\n",
      "Step 50, Loss: 8.487454414367676\n",
      "Step 60, Loss: 8.492654800415039\n",
      "Step 70, Loss: 8.471665382385254\n",
      "Step 80, Loss: 8.395373344421387\n",
      "Step 90, Loss: 8.335044860839844\n",
      "Step 100, Loss: 8.50654411315918\n",
      "Step 110, Loss: 8.480659484863281\n",
      "Step 120, Loss: 8.349784851074219\n",
      "Step 130, Loss: 8.456659317016602\n",
      "Step 140, Loss: 8.470670700073242\n",
      "Step 150, Loss: 8.349361419677734\n",
      "Step 160, Loss: 8.467498779296875\n",
      "Step 170, Loss: 8.335304260253906\n",
      "Step 180, Loss: 8.342676162719727\n",
      "Step 190, Loss: 8.344925880432129\n",
      "Step 200, Loss: 8.42776870727539\n",
      "Step 210, Loss: 8.317412376403809\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 125\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 125\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/liuzhiming/.miniconda3/envs/text-completor/lib/python3.11/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/data/liuzhiming/.miniconda3/envs/text-completor/lib/python3.11/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/data/liuzhiming/.miniconda3/envs/text-completor/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/data/liuzhiming/.miniconda3/envs/text-completor/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[1], line 82\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     80\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_ids)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 82\u001b[0m     mask_start \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mmask_token_id)\n\u001b[1;32m     83\u001b[0m     mask_end \u001b[38;5;241m=\u001b[39m mask_start \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(missing_text\u001b[38;5;241m.\u001b[39msplit())\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(mask_start, mask_end):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pandas as pd\n",
    "\n",
    "# 检查可用的GPU设备\n",
    "if torch.cuda.is_available():\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {num_gpus}\")\n",
    "    for i in range(num_gpus):\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    device = torch.device(\"cuda:0\")  # 使用第一个可用的GPU设备\n",
    "else:\n",
    "    print(\"No GPU devices available. Using CPU.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# 加载预训练模型和分词器\n",
    "model_name = \"/data/modelscope/Qwen2.5-0.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 确保分词器支持 [MASK] 令牌\n",
    "if tokenizer.mask_token is None:\n",
    "    tokenizer.add_special_tokens({\"mask_token\": \"[MASK]\"})\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# 调整模型以适应新的特殊令牌\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# 设置设备\n",
    "model.to(device)\n",
    "\n",
    "# 定义LORA配置\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # 减少 r 的值\n",
    "    lora_alpha=16,  # 减少 lora_alpha 的值\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,  # 减少 dropout 的值\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "# 将LORA应用到模型上\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(model.print_trainable_parameters())\n",
    "\n",
    "\n",
    "# 自定义数据集类\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, examples, tokenizer, max_length=512):\n",
    "        self.examples = []\n",
    "        for example in examples:\n",
    "            parts = example.strip().split(\"\\t\")\n",
    "            if len(parts) == 2:\n",
    "                context_before_mask, missing_text_and_context_after = parts\n",
    "                context_before, _, context_after = context_before_mask.partition(\"[MASK]\")\n",
    "                if \" \" in missing_text_and_context_after:\n",
    "                    missing_text, context_after = missing_text_and_context_after.split(\" \", 1)\n",
    "                    self.examples.append((context_before, missing_text, context_after))\n",
    "                else:\n",
    "                    print(f\"Skipping malformed line: {example.strip()}\")\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context_before, missing_text, context_after = self.examples[idx]\n",
    "\n",
    "        # 创建完整的上下文\n",
    "        full_context = f\"{context_before} [MASK] {context_after}\"\n",
    "        input_ids = self.tokenizer.encode(\n",
    "            full_context, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\"\n",
    "        ).squeeze()\n",
    "\n",
    "        # 创建标签（即中间缺失的文本）\n",
    "        labels = [-100] * len(input_ids)\n",
    "        try:\n",
    "            mask_start = input_ids.tolist().index(self.tokenizer.mask_token_id)\n",
    "            mask_end = mask_start + len(missing_text.split())\n",
    "            for i in range(mask_start, mask_end):\n",
    "                labels[i] = input_ids[i]\n",
    "        except ValueError:\n",
    "            print(f\"Mask token not found in example: {full_context}\")\n",
    "            raise\n",
    "\n",
    "        attention_mask = (input_ids != self.tokenizer.pad_token_id).long()\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": torch.tensor(labels)}\n",
    "\n",
    "\n",
    "# 加载自定义数据集\n",
    "dataset_path = \"train2_10k.txt\"\n",
    "\n",
    "# 分割数据集为训练集和验证集\n",
    "with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "train_lines = lines[: int(0.8 * len(lines))]\n",
    "eval_lines = lines[int(0.8 * len(lines)) :]\n",
    "\n",
    "train_dataset = CustomDataset(train_lines, tokenizer)\n",
    "eval_dataset = CustomDataset(eval_lines, tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=8)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=8)\n",
    "\n",
    "# 设置优化器和学习率调度器\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-5)  # 尝试更低的学习率\n",
    "num_epochs = 5  # 增加训练轮数\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# 初始化 TensorBoard 写入器\n",
    "writer = SummaryWriter(log_dir=\"./runs/lora_qwen_2.5-0.5B_custom_fill_gpu0\")\n",
    "\n",
    "# 训练循环\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print(f\"Step {step}, Loss: {loss.item()}\")\n",
    "            writer.add_scalar(\"Training Loss\", loss.item(), epoch * len(train_dataloader) + step)\n",
    "\n",
    "# 关闭 TensorBoard 写入器\n",
    "writer.close()\n",
    "\n",
    "# 评估模型\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "with torch.no_grad():\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        total_loss += outputs.loss.item()\n",
    "\n",
    "avg_eval_loss = total_loss / len(eval_dataloader)\n",
    "print(f\"Evaluation Loss: {avg_eval_loss}\")\n",
    "\n",
    "# 保存模型\n",
    "model.save_pretrained(\"./lora_qwen_2.5-0.5B_custom_fill_gpu0\")\n",
    "tokenizer.save_pretrained(\"./lora_qwen_2.5-0.5B_custom_fill_gpu0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-completor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
